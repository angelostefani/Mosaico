import os
import uuid
import requests
import json
import sqlite3
import threading
import re
from datetime import datetime
from typing import Optional, List, Tuple, Dict, Any
from contextlib import contextmanager
from dotenv import load_dotenv
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import pdfplumber
import docx
try:
    # Import pesante; lo carichiamo solo se disponibile
    from sentence_transformers import SentenceTransformer  # type: ignore
except Exception:  # ImportError o altri errori during import
    SentenceTransformer = None  # type: ignore
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance, PointStruct
from qdrant_client.http.exceptions import ResponseHandlingException
from httpx import ConnectError as HTTPXConnectError
from httpcore import ConnectError as HTTPCoreConnectError
from fastapi.staticfiles import StaticFiles
from fastapi import HTTPException, Query
from fastapi import Depends, HTTPException, Header
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import logging
import tempfile
import time
from difflib import SequenceMatcher

logger = logging.getLogger("uvicorn.error")

# Caricamento variabili d'ambiente da file .env e sistema
load_dotenv()

# Configurazione dell'app via python-dotenv e os.getenv
UPLOAD_DIR = os.getenv('UPLOAD_DIR', './uploads')
UPLOAD_DB_PATH = os.getenv('UPLOAD_DB_PATH', os.path.join(UPLOAD_DIR, 'uploads.sqlite3'))
QDRANT_HOST = os.getenv('QDRANT_HOST', 'localhost')
QDRANT_PORT = int(os.getenv('QDRANT_PORT', 6333))
OLLAMA_URL = os.getenv('OLLAMA_URL', 'http://localhost:11434/api/generate')
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'gpt-oss:20b')
EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL', 'all-MiniLM-L6-v2')
CHUNK_SIZE = int(os.getenv('CHUNK_SIZE', 500))
CHUNK_OVERLAP = int(os.getenv('CHUNK_OVERLAP', 50))
QDRANT_SCORE_THRESHOLD = float(os.getenv('QDRANT_SCORE_THRESHOLD', "0.3"))
ENABLE_RAG_DEBUG = os.getenv('ENABLE_RAG_DEBUG', 'false').lower() in ('1', 'true', 'yes')
ENABLE_RERANK = os.getenv('ENABLE_RERANK', 'true').lower() in ('1', 'true', 'yes')
CHAT_RESULT_LIMIT = int(os.getenv('CHAT_RESULT_LIMIT', 10))

if CHUNK_OVERLAP < 0:
    CHUNK_OVERLAP = 0
if QDRANT_SCORE_THRESHOLD < 0:
    QDRANT_SCORE_THRESHOLD = 0.0
if CHAT_RESULT_LIMIT <= 0:
    CHAT_RESULT_LIMIT = 10

_embedding_model_lock = threading.Lock()
_embedding_model_instance: Optional["SentenceTransformer"] = None  # type: ignore[name-defined]
_embedding_model_failed = False
_fallback_logging_emitted = False

# Creazione cartella upload se non esiste
os.makedirs(UPLOAD_DIR, exist_ok=True)
db_dir = os.path.dirname(UPLOAD_DB_PATH)
if db_dir and not os.path.exists(db_dir):
    os.makedirs(db_dir, exist_ok=True)



def _utc_now_iso() -> str:
    """Restituisce timestamp UTC ISO-8601 (secondi)."""
    return datetime.utcnow().replace(microsecond=0).isoformat() + "Z"


@contextmanager
def get_upload_db_conn():
    """Connessione SQLite con row_factory=sqlite3.Row."""
    conn = sqlite3.connect(UPLOAD_DB_PATH, check_same_thread=False)
    conn.row_factory = sqlite3.Row
    try:
        yield conn
    finally:
        conn.close()


def init_upload_db() -> None:
    """Crea la tabella uploads se non esiste."""
    with get_upload_db_conn() as conn:
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS uploads (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                upload_id TEXT UNIQUE NOT NULL,
                username TEXT,
                collection TEXT,
                original_filename TEXT,
                stored_filename TEXT,
                file_path TEXT,
                size_bytes INTEGER,
                num_chunks INTEGER,
                num_points INTEGER,
                status TEXT NOT NULL,
                error_message TEXT,
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL
            )
            """
        )
        conn.commit()


def create_upload_record(
    upload_id: str,
    username: Optional[str],
    collection: Optional[str],
    original_filename: str,
    stored_filename: str,
    file_path: str,
    size_bytes: int,
) -> None:
    now = _utc_now_iso()
    with get_upload_db_conn() as conn:
        conn.execute(
            """
            INSERT INTO uploads (
                upload_id, username, collection, original_filename, stored_filename,
                file_path, size_bytes, status, created_at, updated_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                upload_id,
                username,
                collection,
                original_filename,
                stored_filename,
                file_path,
                size_bytes,
                "processing",
                now,
                now,
            ),
        )
        conn.commit()


def update_upload_record(upload_id: str, **fields) -> None:
    if not fields:
        return
    fields["updated_at"] = _utc_now_iso()
    assignments = ", ".join(f"{key} = ?" for key in fields.keys())
    params = list(fields.values()) + [upload_id]
    with get_upload_db_conn() as conn:
        conn.execute(
            f"UPDATE uploads SET {assignments} WHERE upload_id = ?",
            params,
        )
        conn.commit()


init_upload_db()

# toggle in sviluppo per saltare la verifica JWT di Django
# Abilita automaticamente in ambiente di test (pytest)
_default_skip = "true" if os.getenv("PYTEST_CURRENT_TEST") else "false"
SKIP_AUTH = os.getenv("SKIP_AUTH", _default_skip).lower() in ("1", "true", "yes")

# HTTP Bearer auth scheme (opzionale quando SKIP_AUTH è attivo)
# auto_error=False evita 403 automatico se manca l'header Authorization
bearer_scheme = HTTPBearer(auto_error=False)

# URL dell’endpoint Django che verifica il token (es. /api/token/verify/)
DJANGO_VERIFY_URL = os.getenv("DJANGO_VERIFY_URL", "http://localhost:8001/api/token/verify/")

async def django_verify_token(
    credentials: Optional[HTTPAuthorizationCredentials] = Depends(bearer_scheme),
):
    # bypass in sviluppo
    if SKIP_AUTH:
        return {"username": "dev-user"}

    if not credentials or not credentials.credentials:
        raise HTTPException(status_code=401, detail="Token mancante")

    token = credentials.credentials
    resp = requests.post(
        DJANGO_VERIFY_URL,
        json={"token": token},
        headers={"Content-Type": "application/json"},
        timeout=5
    )
    if resp.status_code != 200:
        raise HTTPException(status_code=401, detail="Token non valido")
    return resp.json()

# Inizializzazione FastAPI
app = FastAPI(
    title="Document QA & Chat API",
    description="API per upload documenti, estrazione, salvataggio in Qdrant e interazione con l'IA",
    version="1.0"
)

# Configurazione CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],         # qualunque origine può fare la fetch
    allow_credentials=False,     # non servono cookie/BasicAuth
    allow_methods=["*"],
    allow_headers=["*"],         # include anche "authorization"
)


_CONTROL_CHARS_RE = re.compile(r"[\u0000-\u0008\u000b-\u000c\u000e-\u001f\u007f]")
_INLINE_WHITESPACE_RE = re.compile(r"[ \t\f\v]+")
_MULTIPLE_NEWLINES_RE = re.compile(r"\n\s*\n+")


def _normalize_text(text: str) -> str:
    """
    Normalizza il testo rimuovendo caratteri di controllo e spaziature eccessive.
    """
    if not text:
        return ""
    normalized = text.replace("\r\n", "\n").replace("\r", "\n")
    normalized = _CONTROL_CHARS_RE.sub(" ", normalized)
    normalized = _INLINE_WHITESPACE_RE.sub(" ", normalized)
    normalized = _MULTIPLE_NEWLINES_RE.sub("\n\n", normalized)
    return normalized.strip()


def chunk_text(
    text: str,
    chunk_size: int = CHUNK_SIZE,
    chunk_overlap: int = CHUNK_OVERLAP,
    return_indices: bool = False,
    normalize: bool = True,
) -> List[Any]:
    """
    Suddivide il testo normalizzato in frammenti con overlap opzionale.
    Restituisce una lista di stringhe oppure (chunk, start, end) se `return_indices=True`.
    """
    text_to_process = _normalize_text(text) if normalize else (text or "")
    if not text_to_process:
        return []

    chunk_size = max(1, chunk_size)
    if chunk_overlap < 0:
        chunk_overlap = 0
    if chunk_overlap >= chunk_size:
        chunk_overlap = chunk_size - 1
    step = max(1, chunk_size - chunk_overlap)

    chunks: List[Any] = []
    for start in range(0, len(text_to_process), step):
        end = min(start + chunk_size, len(text_to_process))
        raw_chunk = text_to_process[start:end]
        if not raw_chunk:
            continue
        stripped_left = len(raw_chunk) - len(raw_chunk.lstrip())
        stripped_right = len(raw_chunk) - len(raw_chunk.rstrip())
        trimmed = raw_chunk.strip()
        if not trimmed:
            continue
        char_start = start + stripped_left
        char_end = end - stripped_right if stripped_right else end

        if return_indices:
            chunks.append((trimmed, char_start, char_end))
        else:
            chunks.append(trimmed)

    return chunks


def _derive_doc_title(normalized_text: str) -> Optional[str]:
    """
    Restituisce la prima riga non vuota del documento come titolo.
    """
    for line in normalized_text.splitlines():
        candidate = line.strip()
        if candidate:
            return candidate[:200]
    return None


def _infer_page_number(char_start: int, char_end: int, pages_meta: List[Dict[str, Any]]) -> Optional[int]:
    """
    Deduce il numero di pagina in base agli offset normalizzati del chunk.
    """
    if not pages_meta:
        return None

    midpoint = char_start + max(0, (char_end - char_start) // 2)
    for page in pages_meta:
        start = page.get("normalized_start")
        end = page.get("normalized_end")
        if start is None or end is None:
            continue
        if start <= midpoint <= end:
            return page.get("page_number")

    if midpoint > (pages_meta[-1].get("normalized_end") or 0):
        return pages_meta[-1].get("page_number")
    return pages_meta[0].get("page_number")


def extract_text(file_location: str, return_metadata: bool = False) -> Any:
    """
    Estrae il testo da file .txt, .pdf, .doc/.docx.
    Se `return_metadata` è True restituisce una tupla (testo, metadata).
    """
    ext = os.path.splitext(file_location)[1].lower()
    metadata: Dict[str, Any] = {"source_ext": ext, "pages": []}

    if ext == ".txt":
        with open(file_location, "r", encoding="utf-8") as f:
            text = f.read()
    elif ext == ".pdf":
        try:
            with pdfplumber.open(file_location) as pdf:
                raw_pages: List[Tuple[int, str]] = []
                for idx, page in enumerate(pdf.pages):
                    page_text = page.extract_text() or ""
                    if page_text.strip():
                        raw_pages.append((idx + 1, page_text))

            page_texts = [page for _, page in raw_pages]
            text = "\n\n".join(page_texts)

            normalized_parts = [_normalize_text(page) for page in page_texts]
            separator_len = 2  # len("\n\n")
            cursor = 0
            pages_meta: List[Dict[str, Any]] = []
            for idx, (page_number, original_page) in enumerate(raw_pages):
                normalized_page = normalized_parts[idx]
                page_entry = {
                    "page_number": page_number,
                    "text_preview": original_page.strip()[:200],
                    "normalized_start": cursor,
                    "normalized_end": cursor + len(normalized_page),
                }
                pages_meta.append(page_entry)
                if idx < len(raw_pages) - 1:
                    cursor += len(normalized_page) + separator_len
                else:
                    cursor += len(normalized_page)

            metadata["pages"] = pages_meta
            metadata["normalized_text"] = "\n\n".join(normalized_parts)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Errore estrazione PDF: {e}")
    elif ext in [".doc", ".docx"]:
        try:
            doc = docx.Document(file_location)
            paras = [para.text for para in doc.paragraphs if para.text]
            text = "\n".join(paras)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Errore estrazione DOCX: {e}")
    else:
        raise HTTPException(status_code=400, detail="Formato file non supportato")

    if return_metadata:
        metadata.setdefault("pages", [])
        if "normalized_text" not in metadata:
            metadata["normalized_text"] = _normalize_text(text)
        return text, metadata

    return text


def init_qdrant_client() -> QdrantClient:
    """
    Inizializza e restituisce il client Qdrant.
    """
    try:
        client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
        return client
    except (HTTPXConnectError, HTTPCoreConnectError, ResponseHandlingException) as e:
        raise HTTPException(status_code=503, detail=f"Impossibile connettersi a Qdrant: {e}")


def get_embedding_model() -> SentenceTransformer:
    """
    Restituisce il modello di embedding. Se sentence-transformers non è
    disponibile, usa un fallback leggero deterministico.
    """
    global _embedding_model_instance, _embedding_model_failed, _fallback_logging_emitted

    if SentenceTransformer is not None and not _embedding_model_failed:
        if _embedding_model_instance is None:
            with _embedding_model_lock:
                if _embedding_model_instance is None:
                    try:
                        _embedding_model_instance = SentenceTransformer(EMBEDDING_MODEL)
                        if ENABLE_RAG_DEBUG:
                            logger.info("Embedding model '%s' caricato con successo.", EMBEDDING_MODEL)
                    except Exception as exc:  # pragma: no cover - logging path
                        _embedding_model_failed = True
                        logger.warning("Impossibile caricare il modello '%s': %s. Uso fallback deterministico.", EMBEDDING_MODEL, exc)
        if _embedding_model_instance is not None:
            return _embedding_model_instance

    class _FallbackEmbedder:
        def encode(self, inputs):
            def enc_one(text: str):
                # Vettore semplice basato su hash deterministico
                h = abs(hash(text))
                # proietta in dimensione fissa (per coerenza fra query/chunk)
                size = 32
                vec = [(h >> (i % 32)) & 0xFF for i in range(size)]
                return [float(v) / 255.0 for v in vec]

            if isinstance(inputs, list):
                return [enc_one(t) for t in inputs]
            return enc_one(inputs)

    if not _fallback_logging_emitted:
        logger.warning("Uso del fallback per gli embedding: sentence-transformers non disponibile.")
        _fallback_logging_emitted = True

    return _FallbackEmbedder()  # type: ignore


def process_document(
    file_location: str,
    username: Optional[str] = None,
    collection: Optional[str] = None,
    upload_id: Optional[str] = None,
    original_filename: Optional[str] = None,
) -> dict:
    """
    Elabora il documento: estrae testo, chunking, embedding e indicizzazione in Qdrant.
    """
    text, metadata = extract_text(file_location, return_metadata=True)
    normalized_text = metadata.get("normalized_text", "")

    if not normalized_text:
        raise HTTPException(status_code=400, detail="Nessun testo estratto")

    chunk_entries: List[Tuple[str, int, int]] = chunk_text(
        normalized_text,
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        return_indices=True,
        normalize=False,
    )
    if not chunk_entries:
        raise HTTPException(status_code=400, detail="Nessun chunk disponibile dopo la normalizzazione")

    chunks = [entry[0] for entry in chunk_entries]
    model = get_embedding_model()
    embeddings = model.encode(chunks)

    # Costruzione nome collection multi-tenant
    username_val = username.strip() if username and username.strip() else None
    collection_val = collection.strip() if collection and collection.strip() else None
    if username_val and collection_val:
        collection_name = f"{username_val}_{collection_val}"
    elif username_val:
        collection_name = username_val
    elif collection_val:
        collection_name = collection_val
    else:
        collection_name = "documents"

    client = init_qdrant_client()
    if not client.collection_exists(collection_name):
        client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=len(embeddings[0]), distance=Distance.COSINE)
        )
    elif ENABLE_RAG_DEBUG:
        try:
            collection_info = client.get_collection(collection_name=collection_name)
            configured_size = None
            config = getattr(collection_info, "config", None)
            params = getattr(config, "params", None) if config else None
            vectors = getattr(params, "vectors", None) if params else None
            if hasattr(vectors, "size"):
                configured_size = vectors.size
            elif isinstance(vectors, dict):
                configured_size = vectors.get("size")
            if configured_size is not None and configured_size != len(embeddings[0]):
                logger.warning(
                    "[process_document] Dimensione embedding (%s) diversa dalla dimensione configurata in Qdrant (%s) per la collection '%s'.",
                    len(embeddings[0]),
                    configured_size,
                    collection_name,
                )
        except Exception as debug_exc:  # pragma: no cover - best effort diagnostica
            logger.debug("Impossibile verificare la dimensione vettori per '%s': %s", collection_name, debug_exc)

    # Genera un upload_id se non fornito esplicitamente
    upload_id_val = upload_id or str(uuid.uuid4())

    pages_meta = metadata.get("pages") or []
    doc_title = _derive_doc_title(normalized_text)
    base_payload: Dict[str, Any] = {
        "source_file": os.path.basename(file_location),
        "username": username_val or None,
        "collection": collection_val or None,
        "upload_id": upload_id_val,
        "original_filename": original_filename,
        "source_ext": metadata.get("source_ext"),
    }
    if doc_title:
        base_payload["doc_title"] = doc_title

    points = []
    for idx, emb in enumerate(embeddings):
        chunk_text_value, char_start, char_end = chunk_entries[idx]
        payload = {
            **base_payload,
            "text_chunk": chunk_text_value,
            "chunk_index": idx,
            "char_start": char_start,
            "char_end": char_end,
        }
        page_number = _infer_page_number(char_start, char_end, pages_meta)
        if page_number is not None:
            payload["page_number"] = page_number

        points.append(PointStruct(
            id=str(uuid.uuid4()),
            vector=emb.tolist() if hasattr(emb, "tolist") else emb,
            payload=payload,
        ))

    if ENABLE_RAG_DEBUG:
        logger.debug(
            "[process_document] collection=%s upload_id=%s chunks=%s overlap=%s doc_title=%s",
            collection_name,
            upload_id_val,
            len(points),
            CHUNK_OVERLAP,
            doc_title,
        )

    client.upload_points(collection_name=collection_name, points=points)

    return {
        "num_chunks": len(chunks),
        "num_points_inserted": len(points),
        "collection_used": collection_name,
        "upload_id": upload_id_val,
        "original_filename": original_filename,
        "stored_filename": os.path.basename(file_location),
        "doc_title": doc_title,
    }


def ask(prompt: str, rag_context: str) -> str:
    """
    Combina contesto e prompt, invia a Ollama e restituisce la risposta.
    """
    if not rag_context.strip() or rag_context.strip().lower() == "nessun contesto rilevante.":
        return "Non ho abbastanza informazioni dal contesto fornito per rispondere con certezza."

    combined = (
        "Sei un assistente che risponde in modo pertinente al contesto fornito.\n"
        "Regole:\n"
        "1. Mantieni la risposta concisa e fedele al contesto.\n"
        "2. Rispondi in italiano.\n\n"
        f"Contesto:\n{rag_context}\n\n"
        f"Domanda:\n{prompt}\n\n"
        "Risposta:"
    )
    if ENABLE_RAG_DEBUG:
        logger.debug(
            "[ask] question='%s' rag_context_preview='%s' combined_prompt_length=%s",
            prompt[:200],
            rag_context[:500],
            len(combined),
        )
    try:
        response = requests.post(
            OLLAMA_URL,
            headers={"Content-Type": "application/json"},
            json={"model": OLLAMA_MODEL, "prompt": combined},
            stream=True
        )
        answer = ''
        for line in response.iter_lines():
            if not line:
                continue
            try:
                chunk = json.loads(line.decode('utf-8')).get('response', '')
                answer += chunk
            except json.JSONDecodeError:
                continue
        return answer
    except requests.RequestException as e:
        raise HTTPException(status_code=503, detail=f"Errore chiamata Ollama: {e}")

@app.get("/", summary="Endpoint root di esempio")
async def root():
    return {"message": "I-NEST_API is running!"}

@app.get(
    "/collection",
    summary="Elenca gli elementi di una collection",
    tags=["Administration"],
    dependencies=[Depends(django_verify_token)]
)
async def list_collection_items(
    username: Optional[str] = Query(None),
    collection: Optional[str] = Query(None),
    limit: int = Query(100, ge=1, description="Numero massimo di elementi da restituire")
):
    # Costruzione nome collection multi-tenant
    username_val = username.strip() if username and username.strip() else None
    collection_val = collection.strip() if collection and collection.strip() else None
    if username_val and collection_val:
        collection_name = f"{username_val}_{collection_val}"
    elif username_val:
        collection_name = username_val
    elif collection_val:
        collection_name = collection_val
    else:
        collection_name = "documents"

    client = init_qdrant_client()
    if not client.collection_exists(collection_name):
        raise HTTPException(status_code=404, detail=f"Collection '{collection_name}' non trovata.")

    try:
        result = client.scroll(
            collection_name=collection_name,
            limit=limit,
            with_payload=True
        )
        points = getattr(result, "result", result)
    except Exception as e:
        logger.error(f"[list_collection_items] Errore Qdrant scroll: {e}", exc_info=True)
        raise HTTPException(status_code=503, detail=f"Errore Qdrant scroll: {e}")

    # Estrai payload correttamente sia da PointStruct che da dict
    items = []
    for p in points:
        # Normalizza in forma {"id": ..., "payload": {...}}
        if hasattr(p, "payload") or hasattr(p, "id"):
            pid = getattr(p, "id", None)
            payload = getattr(p, "payload", None)
            items.append({"id": pid, "payload": payload})
        elif isinstance(p, dict):
            items.append({"id": p.get("id"), "payload": p.get("payload")})
        else:
            items.append({"id": None, "payload": p})

    # Calcola il totale elementi nella collection (se possibile)
    total_count = None
    try:
        cnt = client.count(collection_name=collection_name, exact=True)
        total_count = getattr(cnt, "count", None)
        if total_count is None and isinstance(cnt, dict):
            total_count = cnt.get("count")
    except Exception as e:
        logger.warning(f"[list_collection_items] Impossibile ottenere il conteggio totale: {e}")

    return {
        "collection": collection_name,
        "count": int(total_count) if total_count is not None else len(items),
        "items": items
    }

@app.post("/upload", summary="Carica documento", tags=["Documenti"], dependencies=[Depends(django_verify_token)])
async def upload_document(
    file: UploadFile = File(...),
    username: Optional[str] = Form(None),
    collection: Optional[str] = Form(None)
):
    ext = os.path.splitext(file.filename)[1]
    unique_name = f"{uuid.uuid4()}{ext}"
    path = os.path.join(UPLOAD_DIR, unique_name)

    try:
        contents = await file.read()
        with open(path, "wb") as f:
            f.write(contents)
        size_bytes = len(contents)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Errore salvataggio file: {e}")

    upload_id = str(uuid.uuid4())
    username_val = username.strip() if username and username.strip() else None
    collection_val = collection.strip() if collection and collection.strip() else None

    try:
        create_upload_record(
            upload_id=upload_id,
            username=username_val,
            collection=collection_val,
            original_filename=file.filename,
            stored_filename=unique_name,
            file_path=path,
            size_bytes=size_bytes,
        )
    except sqlite3.Error as e:
        raise HTTPException(status_code=500, detail=f"Errore database upload: {e}")

    try:
        info = process_document(
            path,
            username,
            collection,
            upload_id=upload_id,
            original_filename=file.filename,
        )
        update_upload_record(
            upload_id,
            status="completed",
            num_chunks=info.get("num_chunks"),
            num_points=info.get("num_points_inserted"),
            collection=info.get("collection_used"),
            error_message=None,
        )
    except Exception as e:
        update_upload_record(upload_id, status="failed", error_message=str(e))
        raise

    return JSONResponse(status_code=200, content={
        "message": "File caricato e processato con successo",
        "upload_id": info.get("upload_id"),
        "processing_info": info
    })


@app.get(
    "/uploads",
    summary="Elenca gli upload registrati",
    tags=["Documenti"],
    dependencies=[Depends(django_verify_token)]
)
async def list_uploads(
    username: Optional[str] = Query(None),
    collection: Optional[str] = Query(None),
    status: Optional[str] = Query(None),
    upload_id: Optional[str] = Query(None),
    limit: int = Query(100, ge=1, le=500, description="Numero massimo di upload da restituire")
):
    filters = []
    params: List[str] = []

    if username and username.strip():
        filters.append("username = ?")
        params.append(username.strip())

    if collection and collection.strip():
        filters.append("collection = ?")
        params.append(collection.strip())

    if status and status.strip():
        filters.append("status = ?")
        params.append(status.strip())

    if upload_id and upload_id.strip():
        filters.append("upload_id = ?")
        params.append(upload_id.strip())

    query = (
        "SELECT upload_id, username, collection, original_filename, stored_filename, file_path, "
        "size_bytes, num_chunks, num_points, status, error_message, created_at, updated_at "
        "FROM uploads"
    )
    if filters:
        query += " WHERE " + " AND ".join(filters)
    query += " ORDER BY datetime(created_at) DESC LIMIT ?"
    params.append(limit)

    with get_upload_db_conn() as conn:
        rows = conn.execute(query, params).fetchall()

    uploads = []
    for row in rows:
        uploads.append({key: row[key] for key in row.keys()})

    return {
        "count": len(uploads),
        "uploads": uploads,
    }


def rerank_results(question: str, results: list, top_k: int = 3) -> list:
    """
    Reranking combinando score vettoriale e similarità testuale.
    """
    scored = []
    question_lower = question.lower()
    
    for r in results:
        payload = getattr(r, "payload", {}) or {}
        text = payload.get("text_chunk", "")
        
        # Score vettoriale (già normalizzato 0-1 da Qdrant cosine)
        vector_score = getattr(r, "score", 0.0)
        
        # Similarità testuale
        text_similarity = SequenceMatcher(None, question_lower, text.lower()).ratio()
        
        # Combina i punteggi (70% vettoriale, 30% testuale)
        combined_score = vector_score * 0.7 + text_similarity * 0.3
        
        scored.append((combined_score, r))
    
    # Ordina per punteggio combinato
    scored.sort(reverse=True, key=lambda x: x[0])
    return [r for _, r in scored[:top_k]]


@app.post("/chat", summary="Interagisci con l'IA", tags=["Chat"], dependencies=[Depends(django_verify_token)])
async def chat(
    question: str = Form(...),
    username: Optional[str] = Form(None),
    collection: Optional[str] = Form(None)
):
    model = get_embedding_model()
    q_emb = model.encode(question).tolist()

    username_val = username.strip() if username and username.strip() else None
    collection_val = collection.strip() if collection and collection.strip() else None
    if username_val and collection_val:
        coll = f"{username_val}_{collection_val}"
    elif username_val:
        coll = username_val
    elif collection_val:
        coll = collection_val
    else:
        coll = "documents"

    client = init_qdrant_client()
    if not client.collection_exists(coll):
        raise HTTPException(status_code=404, detail=f"Collection '{coll}' non trovata.")

    try:
        results = client.search(
            collection_name=coll,
            query_vector=q_emb,
            limit=CHAT_RESULT_LIMIT,
            with_payload=True
        )
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"Errore ricerca in Qdrant: {e}")

    results = results or []
    threshold = max(0.0, QDRANT_SCORE_THRESHOLD)
    filtered_results = []
    debug_snapshots = []

    for r in results:
        payload = getattr(r, "payload", {}) or {}
        score = getattr(r, "score", None)
        include = score is None or score >= threshold
        if include:
            filtered_results.append(r)
        if ENABLE_RAG_DEBUG:
            debug_snapshots.append({
                "score": score,
                "chunk_index": payload.get("chunk_index"),
                "page_number": payload.get("page_number"),
            })

    if ENABLE_RAG_DEBUG:
        logger.debug(
            "[chat] collection=%s threshold=%.3f tot_results=%s used_results=%s debug=%s",
            coll,
            threshold,
            len(results),
            len(filtered_results),
            debug_snapshots,
        )

    reranked_results = filtered_results
    if ENABLE_RERANK and filtered_results:
        reranked_results = rerank_results(
            question,
            filtered_results,
            top_k=min(CHAT_RESULT_LIMIT, len(filtered_results)),
        )

    reranked_results = reranked_results[:CHAT_RESULT_LIMIT]

    if threshold > 0.0 and not reranked_results and results:
        context = "Nessun contesto rilevante."
    elif reranked_results:
        segments = []
        for r in reranked_results:
            payload = getattr(r, "payload", {}) or {}
            chunk_text_value = payload.get("text_chunk", "")
            chunk_index = payload.get("chunk_index")
            prefix = f"[chunk {chunk_index}] " if chunk_index is not None else ""
            segments.append(f"{prefix}{chunk_text_value}".strip())
        context = "\n\n".join(filter(None, segments))
    else:
        context = "Nessun contesto rilevante."

    if ENABLE_RAG_DEBUG:
        logger.debug(
            "[chat] question='%s' context_preview='%s' filtered_results=%s reranked_order=%s limit=%s",
            question[:200],
            context[:500],
            debug_snapshots,
            [
                {
                    "chunk_index": getattr(r, "payload", {}).get("chunk_index") if hasattr(r, "payload") else None,
                    "score": getattr(r, "score", None),
                }
                for r in reranked_results
            ],
            CHAT_RESULT_LIMIT,
        )

    answer = ask(question, context)
    return JSONResponse(status_code=200, content={"message": answer})

@app.get(
    "/collections",
    summary="Elenca tutte le collections",
    tags=["Administration"],
    dependencies=[Depends(django_verify_token)]
)
async def list_collections():
    """Restituisce l'elenco delle collections presenti in Qdrant."""
    client = init_qdrant_client()
    try:
        resp = client.get_collections()
    except Exception as e:
        logger.error(f"[list_collections] Errore Qdrant get_collections: {e}", exc_info=True)
        raise HTTPException(status_code=503, detail=f"Errore Qdrant get_collections: {e}")

    # Normalizza il risultato in una lista di nomi
    collections = []
    data = None
    if hasattr(resp, "collections"):
        data = getattr(resp, "collections")
    elif isinstance(resp, dict):
        data = resp.get("collections")
    else:
        data = resp

    data = data or []
    for c in data:
        if isinstance(c, dict):
            name = c.get("name") or c.get("collection_name")
        else:
            name = getattr(c, "name", None)
        collections.append(name if name else str(c))

    return {
        "count": len(collections),
        "collections": collections
    }

@app.delete("/collection", summary="Elimina una collection", tags=["Administration"], dependencies=[Depends(django_verify_token)])
async def delete_collection(
    username: Optional[str] = Query(None),
    collection: Optional[str] = Query(None),
):
    # Costruzione nome collection multi-tenant (stesso schema di upload/chat)
    username_val = username.strip() if username and username.strip() else None
    collection_val = collection.strip() if collection and collection.strip() else None
    if username_val and collection_val:
        collection_name = f"{username_val}_{collection_val}"
    elif username_val:
        collection_name = username_val
    elif collection_val:
        collection_name = collection_val
    else:
        collection_name = "documents"

    client = init_qdrant_client()
    if not client.collection_exists(collection_name):
        raise HTTPException(status_code=404, detail=f"Collection '{collection_name}' non trovata.")

    client.delete_collection(collection_name=collection_name)
    return {"message": f"Collection '{collection_name}' eliminata con successo."}

app.mount("/static", StaticFiles(directory="frontend", html=True), name="static")


@app.get("/healthz", summary="Stato servizi e config", tags=["Administration"])
async def healthz():
    qdrant_status = {"ok": False, "error": None, "latency_ms": None}
    ollama_status = {"ok": False, "error": None, "latency_ms": None}
    storage_status = {"upload_dir": UPLOAD_DIR, "writable": False, "error": None}

    # Check Qdrant
    t0 = time.time()
    try:
        client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
        # chiamata leggera per verificare disponibilità
        _ = client.get_collections()
        qdrant_status["ok"] = True
    except Exception as e:
        qdrant_status["error"] = str(e)
    finally:
        qdrant_status["latency_ms"] = int((time.time() - t0) * 1000)

    # Check Ollama (usa endpoint /api/tags se disponibile)
    def _ollama_tags_url() -> str:
        if "/api/" in OLLAMA_URL:
            base = OLLAMA_URL.split("/api/")[0]
            return f"{base}/api/tags"
        return OLLAMA_URL.rstrip("/") + "/api/tags"

    t1 = time.time()
    try:
        tags_url = _ollama_tags_url()
        resp = requests.get(tags_url, timeout=3)
        if resp.status_code == 200:
            ollama_status["ok"] = True
        else:
            ollama_status["error"] = f"HTTP {resp.status_code}"
    except Exception as e:
        ollama_status["error"] = str(e)
    finally:
        ollama_status["latency_ms"] = int((time.time() - t1) * 1000)

    # Check scrittura su upload dir
    try:
        os.makedirs(UPLOAD_DIR, exist_ok=True)
        with tempfile.NamedTemporaryFile(dir=UPLOAD_DIR, delete=True) as _:
            pass
        storage_status["writable"] = True
    except Exception as e:
        storage_status["error"] = str(e)

    overall_ok = qdrant_status["ok"] and ollama_status["ok"] and storage_status["writable"]

    body = {
        "status": "ok" if overall_ok else "degraded",
        "qdrant": qdrant_status,
        "ollama": ollama_status,
        "storage": storage_status,
        "config": {
            "qdrant_host": QDRANT_HOST,
            "qdrant_port": QDRANT_PORT,
            "ollama_url": OLLAMA_URL,
            "ollama_model": OLLAMA_MODEL,
            "embedding_model": EMBEDDING_MODEL,
            "chunk_size": CHUNK_SIZE,
            "chunk_overlap": CHUNK_OVERLAP,
            "qdrant_score_threshold": QDRANT_SCORE_THRESHOLD,
            "enable_rag_debug": ENABLE_RAG_DEBUG,
            "enable_rerank": ENABLE_RERANK,
            "chat_result_limit": CHAT_RESULT_LIMIT,
            "skip_auth": SKIP_AUTH,
        },
    }

    return JSONResponse(status_code=(200 if overall_ok else 503), content=body)

if __name__ == "__main__":   
    import uvicorn
    uvicorn.run("app:app", host="0.0.0.0", port=int(os.getenv('PORT', 9000)), reload=True)
    
